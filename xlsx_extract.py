# -*- coding: utf-8 -*-
import sqlite3
from typing import List, Dict, Optional, Iterable
import pandas as pd
import os
import requests

# ============ ì‚¬ìš©ì ì„¤ì • ============
EXCEL_PATH = "ë†ì¶•ìˆ˜ì‚°ë¬¼ í’ˆëª© ë° ë“±ê¸‰ ì½”ë“œí‘œ.xlsx"
SQLITE_PATH = "kamis_api_list.db"
SQLITE_TABLE = "api_items"

DOWNLOAD_URL = (
    "https://www.kamis.or.kr/customer/board/board_file.do"
    "?brdno=4&brdctsno=424245&brdctsfileno=15636"
)
DOWNLOAD_HEADERS = {
    "User-Agent": "Mozilla/5.0",
    "Referer": "https://www.kamis.or.kr/customer/board/board.do",
}

# ì‹œíŠ¸ëª…
SHEET_BURYU = "ë¶€ë¥˜ì½”ë“œ"
SHEET_PUMMOK = "í’ˆëª©ì½”ë“œ"
SHEET_PUMJONG = "í’ˆì¢…ì½”ë“œ"
SHEET_RANK = "ë“±ê¸‰ì½”ë“œ"
SHEET_LIVESTOCK = "ì¶•ì‚°ë¬¼ ì½”ë“œ"
SHEET_SANMUL = "ì‚°ë¬¼ì½”ë“œ"

# ============ ìŠ¤í‚¤ë§ˆ ë§¤í•‘(ìµœì¢… DB ìŠ¤í‚¤ë§ˆ) ============
# ìµœì¢… DB ì»¬ëŸ¼ëª…(=ê°’ì„ ë„£ì„ ë³€ìˆ˜ëª…)
SCHEMA_MAPPING: Dict[str, str] = {
    "ë¶€ë¥˜ì½”ë“œ": "category_code",
    "ë¶€ë¥˜ëª…": "category_name",
    "í’ˆëª©ì½”ë“œ": "product_code",
    "í’ˆëª©ëª…": "product_name",
    "í’ˆëª©ì½”ë“œ(itemcode)": "item_code",
    "í’ˆì¢…ì½”ë“œ": "kind_code",
    "í’ˆì¢…ëª…": "kind_name",
    "ë“±ê¸‰ì½”ë“œ(p_productrankcode)": "productrank_code",
    "ë“±ê¸‰ì½”ë“œ(p_graderank)": "graderank_code",
    "ë“±ê¸‰ì½”ë“œëª…": "rank_name",
    "ë“±ê¸‰ì½”ë“œ(periodProductList)": "p_periodProductList",
    "ë“±ê¸‰ì½”ë“œ(periodProductName)": "p_periodProductName",
}

# DB í…Œì´ë¸” ì •ì˜(ëª¨ë‘ TEXT) - ì¶•ì‚°ë¬¼ ë“±ê¸‰ëª… ì»¬ëŸ¼ ì¶”ê°€
SQLITE_SCHEMA: Dict[str, str] = {
    "category_code": "TEXT",
    "category_name": "TEXT",
    "product_code": "TEXT",
    "product_name": "TEXT",
    "item_code": "TEXT",
    "item_name": "TEXT",
    "kind_code": "TEXT",
    "kind_name": "TEXT",
    "productrank_code": "TEXT",
    "graderank_code": "TEXT",
    "rank_name": "TEXT",
    "p_periodProductList": "TEXT",
    "p_periodProductName": "TEXT",
}


# ============ ìœ í‹¸ ============
def _strip_str(x):
    return x.strip() if isinstance(x, str) else x


def _drop_all_empty_rows(df: pd.DataFrame) -> pd.DataFrame:
    df = df.map(_strip_str).replace("", pd.NA)
    return df.dropna(how="all")


def _find_header_row(
    df_raw: pd.DataFrame, required_cols: Iterable[str], max_scan: int = 50
) -> Optional[int]:
    req = set(required_cols)
    for i in range(min(max_scan, len(df_raw))):
        headers = [
            str(v).strip() if pd.notna(v) else "" for v in df_raw.iloc[i].tolist()
        ]
        hdr_set = set(h for h in headers if h)
        if req.issubset(hdr_set):
            return i
    return None


def read_sheet_with_header(
    excel_path: str, sheet_name: str, wanted_cols: List[str], max_scan: int = 50
) -> pd.DataFrame:
    df_raw = pd.read_excel(
        excel_path, sheet_name=sheet_name, header=None, engine="openpyxl"
    )
    header_row = _find_header_row(df_raw, wanted_cols, max_scan=max_scan)
    if header_row is None:
        raise ValueError(
            f"[{sheet_name}] í—¤ë” í–‰ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í•„ìš”í•œ ì»¬ëŸ¼: {wanted_cols}"
        )

    df = pd.read_excel(
        excel_path, sheet_name=sheet_name, header=header_row, engine="openpyxl"
    )
    df.columns = [str(c).strip() for c in df.columns]
    df = df.map(_strip_str)

    for col in wanted_cols:
        if col not in df.columns:
            df[col] = pd.NA
    df = df[wanted_cols]
    return _drop_all_empty_rows(df)


# ============ ì‹œíŠ¸ë³„ ë°ì´í„° ì¶”ì¶œ ============
def extract_buryu(path: str) -> pd.DataFrame:
    return read_sheet_with_header(path, SHEET_BURYU, ["ë¶€ë¥˜ì½”ë“œ", "ë¶€ë¥˜ëª…"])


def extract_pummok(path: str) -> pd.DataFrame:
    return read_sheet_with_header(
        path, SHEET_PUMMOK, ["ë¶€ë¥˜ì½”ë“œ", "í’ˆëª©ì½”ë“œ", "í’ˆëª©ëª…"]
    )


def extract_pumjong(path: str) -> pd.DataFrame:
    wanted = ["í’ˆëª© ì½”ë“œ", "í’ˆì¢…ì½”ë“œ", "í’ˆëª©ëª…", "í’ˆì¢…ëª…"]
    df = read_sheet_with_header(path, SHEET_PUMJONG, wanted)
    # í‘œì¤€í™”: "í’ˆëª© ì½”ë“œ" -> "í’ˆëª©ì½”ë“œ"
    df = df.rename(columns={"í’ˆëª© ì½”ë“œ": "í’ˆëª©ì½”ë“œ"})
    return df[["í’ˆëª©ì½”ë“œ", "í’ˆëª©ëª…", "í’ˆì¢…ì½”ë“œ", "í’ˆì¢…ëª…"]]


def extract_rank(path: str) -> pd.DataFrame:
    return read_sheet_with_header(
        path,
        SHEET_RANK,
        ["ë“±ê¸‰ì½”ë“œ(p_productrankcode)", "ë“±ê¸‰ì½”ë“œ(p_graderank)", "ë“±ê¸‰ì½”ë“œëª…"],
    )


def extract_livestock(path: str) -> pd.DataFrame:
    wanted = [
        "í’ˆëª©ëª…",
        "í’ˆì¢…ëª…",
        "ë“±ê¸‰ëª…",
        "í’ˆëª©ì½”ë“œ(itemcode)",
        "í’ˆì¢…ì½”ë“œ(kindcode)",
        "ë“±ê¸‰ì½”ë“œ(periodProductList)",
    ]
    df = read_sheet_with_header(path, SHEET_LIVESTOCK, wanted)

    # â˜… ì¶•ì‚°ë¬¼ ì‹œíŠ¸ì— ë¶€ë¥˜ì½”ë“œ '500'ê³¼ ë¶€ë¥˜ëª… 'ì¶•ì‚°ë¬¼' ì¶”ê°€
    df["ë¶€ë¥˜ì½”ë“œ"] = "500"
    df["ë¶€ë¥˜ëª…"] = "ì¶•ì‚°ë¬¼"

    # ì»¬ëŸ¼ëª… í‘œì¤€í™” - ë“±ê¸‰ëª…ì„ ë“±ê¸‰ì½”ë“œ(periodProductName)ìœ¼ë¡œ ë³€ê²½!
    rename_map = {
        "í’ˆëª©ì½”ë“œ(itemcode)": "í’ˆëª©ì½”ë“œ",
        "í’ˆì¢…ì½”ë“œ(kindcode)": "í’ˆì¢…ì½”ë“œ",
        "ë“±ê¸‰ì½”ë“œ(periodProductList)": "ë“±ê¸‰ì½”ë“œ(periodProductList)",
        "ë“±ê¸‰ëª…": "ë“±ê¸‰ì½”ë“œ(periodProductName)",  # â† ë³€ê²½!
    }
    df = df.rename(columns=rename_map)

    return df[
        [
            "ë¶€ë¥˜ì½”ë“œ",
            "ë¶€ë¥˜ëª…",
            "í’ˆëª©ì½”ë“œ",
            "í’ˆëª©ëª…",
            "í’ˆì¢…ì½”ë“œ",
            "í’ˆì¢…ëª…",
            "ë“±ê¸‰ì½”ë“œ(periodProductList)",
            "ë“±ê¸‰ì½”ë“œ(periodProductName)",
        ]
    ]


def extract_sanmul(path: str) -> pd.DataFrame:
    wanted = [
        "í’ˆëª©ë¶„ë¥˜ëª…",
        "í’ˆëª©ë¶„ë¥˜ì½”ë“œ",
        "í’ˆëª©ëª…",
        "í’ˆëª©ì½”ë“œ",
        "í’ˆì¢…ëª…",
        "í’ˆì¢…ì½”ë“œ",
        "ì‚°ë¬¼ë“±ê¸‰ëª…",
        "ì‚°ë¬¼ë“±ê¸‰ì½”ë“œ",
    ]
    df = read_sheet_with_header(path, SHEET_SANMUL, wanted)
    # í‘œì¤€í™” rename
    rename_map = {
        "í’ˆëª©ë¶„ë¥˜ì½”ë“œ": "ë¶€ë¥˜ì½”ë“œ",
        "í’ˆëª©ë¶„ë¥˜ëª…": "ë¶€ë¥˜ëª…",
        "ì‚°ë¬¼ë“±ê¸‰ëª…": "ë“±ê¸‰ì½”ë“œëª…",
        "ì‚°ë¬¼ë“±ê¸‰ì½”ë“œ": "ë“±ê¸‰ì½”ë“œ(p_productrankcode)",
    }
    df = df.rename(columns=rename_map)
    keep = [
        "ë¶€ë¥˜ì½”ë“œ",
        "ë¶€ë¥˜ëª…",
        "í’ˆëª©ì½”ë“œ",
        "í’ˆëª©ëª…",
        "í’ˆì¢…ì½”ë“œ",
        "í’ˆì¢…ëª…",
        "ë“±ê¸‰ì½”ë“œ(p_productrankcode)",
        "ë“±ê¸‰ì½”ë“œëª…",
    ]
    for c in keep:
        if c not in df.columns:
            df[c] = pd.NA
    return df[keep]


# ============ ë°ì´í„° íƒ€ì… í†µì¼ í•¨ìˆ˜ ============
def normalize_data_types(df: pd.DataFrame) -> pd.DataFrame:
    """ì¡°ì¸ í‚¤ ì»¬ëŸ¼ë“¤ì˜ ë°ì´í„° íƒ€ì…ì„ ë¬¸ìì—´ë¡œ í†µì¼"""
    key_columns = ["ë¶€ë¥˜ì½”ë“œ", "í’ˆëª©ì½”ë“œ", "í’ˆì¢…ì½”ë“œ"]

    for col in key_columns:
        if col in df.columns:
            # NaNì´ ì•„ë‹Œ ê°’ë“¤ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ê³  ì•ë’¤ ê³µë°± ì œê±°
            df[col] = df[col].astype(str).str.strip()
            # 'nan' ë¬¸ìì—´ì„ ë‹¤ì‹œ NaNìœ¼ë¡œ ë³€í™˜
            df[col] = df[col].replace("nan", pd.NA)

    return df


# ============ ê´€ê³„í˜• ë³‘í•© ë¡œì§ ============
def merge_hierarchically(path: str) -> pd.DataFrame:
    """ê³„ì¸µêµ¬ì¡°ë¥¼ ë”°ë¼ ê´€ê³„í˜• ì¡°ì¸ìœ¼ë¡œ ë°ì´í„° ë³‘í•©"""

    # 1. ê° ì‹œíŠ¸ë³„ ë°ì´í„° ì¶”ì¶œ
    df_buryu = extract_buryu(path)
    df_pummok = extract_pummok(path)
    df_pumjong = extract_pumjong(path)
    df_rank = extract_rank(path)
    df_livestock = extract_livestock(path)
    df_sanmul = extract_sanmul(path)

    # â˜… ë°ì´í„° íƒ€ì… í†µì¼ (ì¡°ì¸ ì „ í•„ìˆ˜!)
    df_buryu = normalize_data_types(df_buryu)
    df_pummok = normalize_data_types(df_pummok)
    df_pumjong = normalize_data_types(df_pumjong)
    df_livestock = normalize_data_types(df_livestock)
    df_sanmul = normalize_data_types(df_sanmul)

    print(f"ë¶€ë¥˜ ì‹œíŠ¸: {len(df_buryu)}í–‰")
    print(f"í’ˆëª© ì‹œíŠ¸: {len(df_pummok)}í–‰")
    print(f"í’ˆì¢… ì‹œíŠ¸: {len(df_pumjong)}í–‰")
    print(f"ë“±ê¸‰ ì‹œíŠ¸: {len(df_rank)}í–‰")
    print(
        f"ì¶•ì‚°ë¬¼ ì‹œíŠ¸: {len(df_livestock)}í–‰ (ë¶€ë¥˜ì½”ë“œ='500', ë¶€ë¥˜ëª…='ì¶•ì‚°ë¬¼' ìë™ ì¶”ê°€)"
    )
    print(f"ì‚°ë¬¼ ì‹œíŠ¸: {len(df_sanmul)}í–‰")

    # 2. ê¸°ë³¸ ê³¨ê²©: ë¶€ë¥˜ â†’ í’ˆëª© ì¡°ì¸
    result = df_buryu.merge(
        df_pummok, on="ë¶€ë¥˜ì½”ë“œ", how="outer", suffixes=("", "_pummok")
    )

    # ë¶€ë¥˜ëª… ë³‘í•© (ë¶€ë¥˜ ì‹œíŠ¸ ìš°ì„ )
    result["ë¶€ë¥˜ëª…"] = result["ë¶€ë¥˜ëª…"].fillna(result.get("ë¶€ë¥˜ëª…_pummok", pd.NA))
    if "ë¶€ë¥˜ëª…_pummok" in result.columns:
        result = result.drop("ë¶€ë¥˜ëª…_pummok", axis=1)

    print(f"ë¶€ë¥˜+í’ˆëª© ì¡°ì¸ í›„: {len(result)}í–‰")

    # 3. í’ˆì¢… ë°ì´í„° ì¡°ì¸ (í’ˆëª©ì½”ë“œ ê¸°ì¤€)
    result = result.merge(
        df_pumjong, on="í’ˆëª©ì½”ë“œ", how="outer", suffixes=("", "_pumjong")
    )

    # í’ˆëª©ëª… ë³‘í•© (ê¸°ì¡´ ë°ì´í„° ìš°ì„ )
    result["í’ˆëª©ëª…"] = result["í’ˆëª©ëª…"].fillna(result.get("í’ˆëª©ëª…_pumjong", pd.NA))
    if "í’ˆëª©ëª…_pumjong" in result.columns:
        result = result.drop("í’ˆëª©ëª…_pumjong", axis=1)

    print(f"í’ˆì¢… ì¡°ì¸ í›„: {len(result)}í–‰")

    # 4. ì¶•ì‚°ë¬¼ ë°ì´í„° ì¡°ì¸ (ë¶€ë¥˜ì½”ë“œ='500' ê¸°ì¤€)
    livestock_merge_cols = ["ë¶€ë¥˜ì½”ë“œ", "í’ˆëª©ì½”ë“œ", "í’ˆì¢…ì½”ë“œ"]
    result = result.merge(
        df_livestock, on=livestock_merge_cols, how="outer", suffixes=("", "_livestock")
    )

    # ì¤‘ë³µ ì»¬ëŸ¼ ë³‘í•© - ì¶•ì‚°ë¬¼ ê³ ìœ  ë“±ê¸‰ ì •ë³´ëŠ” ë³„ë„ ì²˜ë¦¬
    for col in ["ë¶€ë¥˜ëª…", "í’ˆëª©ëª…", "í’ˆì¢…ëª…"]:
        livestock_col = f"{col}_livestock"
        if livestock_col in result.columns:
            result[col] = result[col].fillna(result[livestock_col])
            result = result.drop(livestock_col, axis=1)

    print(f"ì¶•ì‚°ë¬¼ ì¡°ì¸ í›„: {len(result)}í–‰")

    # 5. ì‚°ë¬¼ ë°ì´í„° ì¡°ì¸
    sanmul_merge_cols = ["ë¶€ë¥˜ì½”ë“œ", "í’ˆëª©ì½”ë“œ", "í’ˆì¢…ì½”ë“œ"]
    result = result.merge(
        df_sanmul, on=sanmul_merge_cols, how="outer", suffixes=("", "_sanmul")
    )

    # ì¤‘ë³µ ì»¬ëŸ¼ ë³‘í•©
    for col in ["ë¶€ë¥˜ëª…", "í’ˆëª©ëª…", "í’ˆì¢…ëª…", "ë“±ê¸‰ì½”ë“œëª…"]:
        sanmul_col = f"{col}_sanmul"
        if sanmul_col in result.columns:
            result[col] = result[col].fillna(result[sanmul_col])
            result = result.drop(sanmul_col, axis=1)

    print(f"ì‚°ë¬¼ ì¡°ì¸ í›„: {len(result)}í–‰")

    # 6. ë“±ê¸‰ ë°ì´í„° ì¡°ì¸
    if not df_rank.empty:
        non_livestock_mask = result["ë¶€ë¥˜ì½”ë“œ"] != "500"
        livestock_data = result[~non_livestock_mask].copy()
        non_livestock_data = result[non_livestock_mask].copy()

        if not non_livestock_data.empty:
            # ğŸ‘‡ [ìˆ˜ì •] cross join ì „ì— ì¶©ëŒí•  ìˆ˜ ìˆëŠ” ì»¬ëŸ¼ë“¤ì„ ë¯¸ë¦¬ ì‚­ì œí•©ë‹ˆë‹¤.
            # ì´ë ‡ê²Œ í•˜ë©´ merge í›„ì— _x, _y ê°™ì€ ì ‘ë¯¸ì‚¬ê°€ ë¶™ëŠ” ê²ƒì„ ë°©ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            cols_to_drop = [
                col for col in df_rank.columns if col in non_livestock_data.columns
            ]
            non_livestock_data_cleaned = non_livestock_data.drop(columns=cols_to_drop)

            # ì •ë¦¬ëœ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ cross joinì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
            non_livestock_with_rank = non_livestock_data_cleaned.merge(
                df_rank, how="cross"
            )
        else:
            non_livestock_with_rank = pd.DataFrame()  # ë¹„ì–´ìˆëŠ” ê²½ìš°ë¥¼ ìœ„í•œ ì²˜ë¦¬

        # ì¶•ì‚°ë¬¼ ë°ì´í„°ì™€ ë¹„ì¶•ì‚°ë¬¼+ë“±ê¸‰ ë°ì´í„° í•©ì¹˜ê¸°
        # ì¶•ì‚°ë¬¼ ë°ì´í„°ì—ëŠ” ì¼ë°˜ ë“±ê¸‰ ì»¬ëŸ¼ì„ ë¹ˆ ê°’ìœ¼ë¡œ ì¶”ê°€
        if not livestock_data.empty:
            for col in df_rank.columns:
                if col not in livestock_data.columns:
                    livestock_data[col] = pd.NA

        # ìµœì¢… ê²°í•©
        if not livestock_data.empty and not non_livestock_with_rank.empty:
            result = pd.concat(
                [livestock_data, non_livestock_with_rank], ignore_index=True
            )
        elif not livestock_data.empty:
            result = livestock_data
        elif not non_livestock_with_rank.empty:
            result = non_livestock_with_rank
        else:
            result = result  # ì›ë³¸ ìœ ì§€

        print(f"ë“±ê¸‰ ì¡°ì¸ í›„: {len(result)}í–‰ (ì¶•ì‚°ë¬¼ì€ ê³ ìœ  ë“±ê¸‰ ì²´ê³„ ì‚¬ìš©)")

    # 7. ë°ì´í„° ì •ì œ
    result = result.map(_strip_str).replace("", pd.NA)
    result = result.drop_duplicates().reset_index(drop=True)

    print(f"ì¤‘ë³µ ì œê±° í›„ ìµœì¢…: {len(result)}í–‰")

    return result


# ============ ìµœì¢… ìŠ¤í‚¤ë§ˆ ë§¤í•‘ ============
def map_to_final_schema(df: pd.DataFrame) -> pd.DataFrame:
    """ìµœì¢… DB ìŠ¤í‚¤ë§ˆë¡œ ë§¤í•‘"""
    out = pd.DataFrame()

    # SCHEMA_MAPPINGì— ë”°ë¼ ì»¬ëŸ¼ ë§¤í•‘
    for src_col, dst_col in SCHEMA_MAPPING.items():
        out[dst_col] = (
            df[src_col].astype("string")
            if src_col in df.columns
            else pd.Series(dtype="string")
        )

    # item_name = í’ˆëª©ëª…ê³¼ ë™ì¼
    out["item_name"] = (
        df["í’ˆëª©ëª…"].astype("string")
        if "í’ˆëª©ëª…" in df.columns
        else pd.Series(dtype="string")
    )

    # ì „ì²´ string í†µì¼ + ì¤‘ë³µ ì œê±°
    for c in out.columns:
        out[c] = out[c].astype("string")

    return out.drop_duplicates().reset_index(drop=True)


# ============ SQLite ê´€ë ¨ ============
def create_sqlite_table(conn: sqlite3.Connection, table: str, schema: Dict[str, str]):
    cols = ", ".join([f'"{k}" {v}' for k, v in schema.items()])
    conn.execute(f'CREATE TABLE IF NOT EXISTS "{table}" ({cols});')


def create_indexes(conn: sqlite3.Connection, table: str):
    idx_cols = [
        ("idx_category_code", "category_code"),
        ("idx_product_code", "product_code"),
        ("idx_item_code", "item_code"),
        ("idx_kind_code", "kind_code"),
        ("idx_productrank_code", "productrank_code"),
        ("idx_graderank_code", "graderank_code"),
    ]
    for name, col in idx_cols:
        conn.execute(f'CREATE INDEX IF NOT EXISTS {name} ON "{table}"("{col}");')


def load_to_sqlite(df: pd.DataFrame, sqlite_path: str, table: str):
    with sqlite3.connect(sqlite_path) as conn:
        # ê¸°ì¡´ í…Œì´ë¸” ì‚­ì œ í›„ ì¬ìƒì„± (replace ëª¨ë“œ)
        conn.execute(f'DROP TABLE IF EXISTS "{table}";')
        create_sqlite_table(conn, table, SQLITE_SCHEMA)
        df.to_sql(table, conn, if_exists="append", index=False)
        create_indexes(conn, table)


def download_if_needed(path: str, url: str, headers: Dict[str, str] | None = None):
    """pathê°€ ì—†ìœ¼ë©´ urlì—ì„œ ë°›ì•„ ì €ì¥í•œë‹¤."""
    if os.path.exists(path):
        return
    h = headers or {}
    r = requests.get(url, headers=h, timeout=60)
    r.raise_for_status()
    with open(path, "wb") as f:
        f.write(r.content)
    print(f"[download] Saved: {path}")


def main():
    print("=== ê´€ê³„í˜• ë³‘í•© ë°©ì‹ìœ¼ë¡œ ë°ì´í„° ì²˜ë¦¬ ì‹œì‘ ===")

    # ğŸ‘‡ ì¶”ê°€: ì—‘ì…€ íŒŒì¼ ì—†ìœ¼ë©´ ë¨¼ì € ë‚´ë ¤ë°›ê¸°
    download_if_needed(EXCEL_PATH, DOWNLOAD_URL, DOWNLOAD_HEADERS)

    # 1. ê´€ê³„í˜• ì¡°ì¸ìœ¼ë¡œ ë°ì´í„° ë³‘í•©
    merged_data = merge_hierarchically(EXCEL_PATH)

    # 2. ìµœì¢… ìŠ¤í‚¤ë§ˆë¡œ ë§¤í•‘
    final_data = map_to_final_schema(merged_data)

    # 3. SQLiteì— ì €ì¥
    load_to_sqlite(final_data, SQLITE_PATH, SQLITE_TABLE)

    print(f"\n[ì™„ë£Œ] SQLite ì ì¬ ì™„ë£Œ!")
    print(f"- íŒŒì¼: {SQLITE_PATH}")
    print(f"- í…Œì´ë¸”: {SQLITE_TABLE}")
    print(f"- ìµœì¢… í–‰ìˆ˜: {len(final_data)}")


if __name__ == "__main__":
    main()
